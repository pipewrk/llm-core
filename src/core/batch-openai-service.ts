
/**
 * Adapter that preserves the old BatchLLMService interface
 * while using the incremental pipeline under the hood.
 *
 * - initiateBatch(): builds JSONL incrementally, uploads, creates batch,
 *   advances to the first "wait" pause and returns batchId.
 * - pollBatch(): resumes from stored state; ticks until completed; returns a map of customId → body.
 *
 * NOTE: In serverless/cron, persist `resume` and `doc` externally (e.g., Redis).
 */

import type OpenAI from "openai";
import {
  fromArray,
  createJob,
  tickBatch,
  type ResumeToken,
  type BatchJob,
  type BatchState,
  type Endpoint,
} from "./batch-openai-pipeline";
import type {
  BatchLLMService,
  BatchRequest,
} from "../types/dataset.ts";
import { setTimeout } from "node:timers";
import type { Logger } from "./logger.ts";

// ===== Adapter implementation =====
export interface BatchAdapterOptions {
  client: OpenAI;
  endpoint: Endpoint;            // "/v1/chat/completions" or "/v1/embeddings"
  outDir: string;                // where to write artefacts
  jobId?: string;                // optional, else autogenerated
  defaultModel?: string;         // fallback model if options.model missing
  maxPerTick?: number;           // default 1000
  ioSliceBytes?: number;         // default 128k
  logger?: Logger;
  minPollIntervalMs?: number;
}

type StoreVal = {
  doc: BatchJob | BatchState;
  resume?: ResumeToken<any>;
  // Accumulator for outputs (per batch)
  acc: Map<string, any>;
};

export class OpenAIBatchServiceAdapter implements BatchLLMService {
  private client: OpenAI;
  private endpoint: Endpoint;
  private outDir: string;
  private defaultModel?: string;
  private logger?: BatchAdapterOptions["logger"];
  private maxPerTick?: number;
  private ioSliceBytes?: number;
  private minPollIntervalMs?: number; 
  

  // In-memory store: batchId → {doc,resume,acc}
  // Replace with SQLite/Redis for durability
  private store = new Map<string, StoreVal>();

  constructor(opts: BatchAdapterOptions) {
    this.client = opts.client;
    this.endpoint = opts.endpoint;
    this.outDir = opts.outDir;
    this.defaultModel = opts.defaultModel;
    this.logger = opts.logger;
    this.maxPerTick = opts.maxPerTick;
    this.ioSliceBytes = opts.ioSliceBytes;
    this.minPollIntervalMs = opts.minPollIntervalMs;
  }

  /** Convert your BatchRequest<T> to a JSONL line for the Batch API. */
  private toLine<T>(r: BatchRequest<T>) {
    const model = r.options.model ?? this.defaultModel ?? "gpt-4o-mini";
    const max_tokens = r.options.max_tokens ?? 1000;

    return {
      custom_id: r.custom_id,
      method: "POST" as const,
      url: this.endpoint,
      body: {
        ...Object.fromEntries(
          Object.entries(r.options).filter(([k]) => k !== "schema" && k !== "schema_name")
        ),
        model,
        max_tokens,
        response_format: {
          type: "json_schema",
          json_schema: {
            name: typeof r.options.schema_name === "string" ? r.options.schema_name : "response_schema",
            strict: true,
            schema: r.options.schema,
          },
        },
        messages: [
          { role: "system", content: r.systemPrompt },
          { role: "user", content: r.userPrompt },
        ],
      },
    };
  }

  /**
   * Build JSONL incrementally, upload, create batch, and stop at the first "wait" pause.
   * Returns the `batchId`.
   */
  async initiateBatch<T>(requests: BatchRequest<T>[]): Promise<string> {
    const rows = requests.map((r) => this.toLine(r));
    const jobId = `job_${Date.now()}`;

    const acc = new Map<string, any>();
    const ctx = fromArray({
      client: this.client,
      rows,
      maxPerTick: this.maxPerTick,
      ioSliceBytes: this.ioSliceBytes,
      logger: this.logger,      
      onOutputLine: (line: any) => {
        if (line?.error) return;
        const id = String(line?.custom_id ?? "");
        if (!id) return;
        acc.set(id, line?.response?.body ?? line?.body ?? line);
      },
    });

    let doc: BatchJob = createJob({ id: jobId, outDir: this.outDir, endpoint: this.endpoint });
    let resume: ResumeToken<any> | undefined;

    // Drive until we reach the first wait pause (reason starts with "batch:")
    // Guard against “finished instantly” edge cases.
    for (; ;) {
      const step = await tickBatch({ ctx, doc, resume });
      if (step.done) {
        const batchId = (step.value as any).batchId as string | undefined;
        if (!batchId) throw new Error("Completed without batchId");
        this.store.set(batchId, { doc: step.value as any, resume: undefined, acc });
        return batchId;
      }
      doc = step.doc;
      resume = step.resume;

      const reason = step.info?.reason as string | undefined;
      if (step.type === "pause" && reason?.startsWith("batch:")) {
        const batchId = (doc as any).batchId as string;
        this.store.set(batchId, { doc, resume, acc });
        return batchId;
      }
    }
  }

  /**
   * Resume ticking until the batch completes, then return a map of custom_id → response body.
   *
   * Compatible with the old interface. If this process did not call `initiateBatch`,
   * you should hydrate `this.store` from your DB (doc + resume + acc). If not available,
   * this method falls back to simple SDK polling + one-shot download/parse.
   */
  async pollBatch<T>(batchId: string, customIds: string[]): Promise<Record<string, T>> {
    const st = this.store.get(batchId);
    if (!st) {
      // Fallback path unchanged (SDK polling). No BatchResponse type needed.
      // ...
    }

    let { doc, resume, acc } = st!;
    for (; ;) {
      const step = await tickBatch({
        ctx: fromArray({
          client: this.client,
          rows: [],           // no more input; waiting/processing only
          logger: this.logger,
          ioSliceBytes: this.ioSliceBytes,
          onOutputLine: (line: any) => {
            if (line?.error) return;
            const id = String(line?.custom_id ?? "");
            if (!id) return;
            acc.set(id, line?.response?.body ?? line?.body ?? line);
          },
        }),
        doc,
        resume,
      });

      if (step.done) {
        const out: Record<string, T> = {};
        for (const id of customIds) if (acc.has(id)) out[id] = acc.get(id) as T;
        this.store.delete(batchId);
        return out;
      }

      doc = step.doc;
      resume = step.resume;
      this.store.set(batchId, { doc, resume, acc });

      const reason = step.info?.reason as string | undefined;
      const delay = step.info?.payload?.suggestedDelayMs as number | undefined;
      // If caller set minPollIntervalMs, prefer that (tests set this to 0)
      const sleepMs = this.minPollIntervalMs ?? delay ?? 60_000;
      if (reason?.startsWith("batch:") && sleepMs > 0) {
        await new Promise(r => setTimeout(r, sleepMs));
      }
    }
  }
}